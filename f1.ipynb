{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/rohanrao/formula-1-world-championship-1950-2020\n",
      "Dataset rohanrao/formula-1-world-championship-1950-2020 downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# this was how i was able to use pyspark i cannot get it to work when i install apache spark onto my mac - jenny\n",
    "# activate .venv environment\n",
    "# windows: .\\venv\\Scripts\\activate\n",
    "# mac: source .venv/bin/activate\n",
    "# then pip install kaggle, pip install pyspark and pip install findspark inside .venv\n",
    "# move kaggle.json file into ~/.kaggle\n",
    "# then you can run data\n",
    "\n",
    "import kaggle\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "dataset = \"rohanrao/formula-1-world-championship-1950-2020\"\n",
    "\n",
    "kaggle.api.dataset_download_files(dataset, path='./', unzip=True)\n",
    "\n",
    "print(f\"Dataset {dataset} downloaded successfully!\")\n",
    "\n",
    " # create spark session\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Driver_Performance_Prediction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "results = \"results.csv\"\n",
    "qualifying = \"qualifying.csv\"\n",
    "lap_times = \"lap_times.csv\"\n",
    "pit_stops = \"pit_stops.csv\"\n",
    "driver_standings = \"driver_standings.csv\"\n",
    "races = \"races.csv\"\n",
    "constructors = \"constructors.csv\"\n",
    "status = \"status.csv\"\n",
    "circuits = \"circuits.csv\" # might not need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+-------------+----+--------------+-------------+------------+\n",
      "|resultId|raceId|driverId|constructorId|grid|final_position|positionOrder|milliseconds|\n",
      "+--------+------+--------+-------------+----+--------------+-------------+------------+\n",
      "|       1|    18|       1|            1|   1|             1|            1|     5690616|\n",
      "|       2|    18|       2|            2|   5|             2|            2|     5696094|\n",
      "|       3|    18|       3|            3|   7|             3|            3|     5698779|\n",
      "|       4|    18|       4|            4|  11|             4|            4|     5707797|\n",
      "|       5|    18|       5|            1|   3|             5|            5|     5708630|\n",
      "|      23|    19|       8|            6|   2|             1|            1|     5478555|\n",
      "|      24|    19|       9|            2|   4|             2|            2|     5498125|\n",
      "|      25|    19|       5|            1|   8|             3|            3|     5517005|\n",
      "|      26|    19|      15|            7|   3|             4|            4|     5524387|\n",
      "|      27|    19|       1|            1|   9|             5|            5|     5525103|\n",
      "|      28|    19|       2|            2|   5|             6|            6|     5528388|\n",
      "|      29|    19|      17|            9|   6|             7|            7|     5546685|\n",
      "|      30|    19|       4|            4|   7|             8|            8|     5548596|\n",
      "|      31|    19|      14|            9|  12|             9|            9|     5554775|\n",
      "|      32|    19|      18|           11|  11|            10|           10|     5564769|\n",
      "|      33|    19|      12|            4|  13|            11|           11|     5570757|\n",
      "|      45|    20|      13|            6|   2|             1|            1|     5466970|\n",
      "|      46|    20|       8|            6|   4|             2|            2|     5470309|\n",
      "|      47|    20|       9|            2|   1|             3|            3|     5471968|\n",
      "|      48|    20|       2|            2|   6|             4|            4|     5475379|\n",
      "+--------+------+--------+-------------+----+--------------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.read.csv(results, header=True, inferSchema=True)\n",
    "\n",
    "# need to remove \\N\n",
    "result_df = result_df.dropna()\n",
    "result_df = result_df.drop('number', 'positionText', 'time', 'rank','statusId', 'fastestLap', 'fastestLapTime','fastestLapSpeed', 'time', 'laps', 'points')\n",
    "result_df = result_df.withColumn('position', col('position').cast(IntegerType()))\n",
    "result_df = result_df.withColumn('milliseconds', col('milliseconds').cast(IntegerType()))\n",
    "result_df = result_df.dropna()\n",
    "result_df = result_df.withColumnRenamed('position', 'final_position')\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------+-------------+-------------------+\n",
      "|qualifyId|raceId|driverId|constructorId|qualifying_position|\n",
      "+---------+------+--------+-------------+-------------------+\n",
      "|        1|    18|       1|            1|                  1|\n",
      "|        2|    18|       9|            2|                  2|\n",
      "|        3|    18|       5|            1|                  3|\n",
      "|        4|    18|      13|            6|                  4|\n",
      "|        5|    18|       2|            2|                  5|\n",
      "|        6|    18|      15|            7|                  6|\n",
      "|        7|    18|       3|            3|                  7|\n",
      "|        8|    18|      14|            9|                  8|\n",
      "|        9|    18|      10|            7|                  9|\n",
      "|       10|    18|      20|            5|                 10|\n",
      "|       11|    18|      22|           11|                 11|\n",
      "|       12|    18|       4|            4|                 12|\n",
      "|       13|    18|      18|           11|                 13|\n",
      "|       14|    18|       6|            3|                 14|\n",
      "|       15|    18|      17|            9|                 15|\n",
      "|       16|    18|       8|            6|                 16|\n",
      "|       17|    18|      21|           10|                 17|\n",
      "|       18|    18|       7|            5|                 18|\n",
      "|       19|    18|      16|           10|                 19|\n",
      "|       20|    18|      11|            8|                 20|\n",
      "+---------+------+--------+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qualifying_df = spark.read.csv(qualifying, header=True, inferSchema=True)\n",
    "qualifying_df = qualifying_df.drop('number', 'q1', 'q2', 'q3')\n",
    "qualifying_df = qualifying_df.withColumnRenamed('position', 'qualifying_position')\n",
    "qualifying_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+------------+--------+\n",
      "|raceId|driverId|lap|lap_position|lap_time|\n",
      "+------+--------+---+------------+--------+\n",
      "|   841|      20|  1|           1|   98109|\n",
      "|   841|      20|  2|           1|   93006|\n",
      "|   841|      20|  3|           1|   92713|\n",
      "|   841|      20|  4|           1|   92803|\n",
      "|   841|      20|  5|           1|   92342|\n",
      "|   841|      20|  6|           1|   92605|\n",
      "|   841|      20|  7|           1|   92502|\n",
      "|   841|      20|  8|           1|   92537|\n",
      "|   841|      20|  9|           1|   93240|\n",
      "|   841|      20| 10|           1|   92572|\n",
      "|   841|      20| 11|           1|   92669|\n",
      "|   841|      20| 12|           1|   92902|\n",
      "|   841|      20| 13|           1|   93698|\n",
      "|   841|      20| 14|           3|  112075|\n",
      "|   841|      20| 15|           4|   98385|\n",
      "|   841|      20| 16|           2|   91548|\n",
      "|   841|      20| 17|           1|   90800|\n",
      "|   841|      20| 18|           1|   91810|\n",
      "|   841|      20| 19|           1|   91018|\n",
      "|   841|      20| 20|           1|   91055|\n",
      "+------+--------+---+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lap_times_df = spark.read.csv(lap_times, header=True, inferSchema=True)\n",
    "lap_times_df = lap_times_df.drop('time')\n",
    "lap_times_df = lap_times_df.withColumnRenamed('milliseconds', 'lap_time').withColumnRenamed('position', 'lap_position')\n",
    "lap_times_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+---+-------------+\n",
      "|raceId|driverId|pit_stop|lap|stop_duration|\n",
      "+------+--------+--------+---+-------------+\n",
      "|   841|     153|       1|  1|       26.898|\n",
      "|   841|      30|       1|  1|       25.021|\n",
      "|   841|      17|       1| 11|       23.426|\n",
      "|   841|       4|       1| 12|       23.251|\n",
      "|   841|      13|       1| 13|       23.842|\n",
      "|   841|      22|       1| 13|       23.643|\n",
      "|   841|      20|       1| 14|       22.603|\n",
      "|   841|     814|       1| 14|       24.863|\n",
      "|   841|     816|       1| 14|       25.259|\n",
      "|   841|      67|       1| 15|       25.342|\n",
      "|   841|       2|       1| 15|       22.994|\n",
      "|   841|       1|       1| 16|       23.227|\n",
      "|   841|     808|       1| 16|       24.535|\n",
      "|   841|       3|       1| 16|       23.716|\n",
      "|   841|     155|       1| 16|       24.064|\n",
      "|   841|      16|       1| 16|       25.978|\n",
      "|   841|      15|       1| 16|       24.899|\n",
      "|   841|      18|       1| 17|       16.867|\n",
      "|   841|     153|       2| 17|       24.463|\n",
      "|   841|       5|       1| 17|       24.865|\n",
      "+------+--------+--------+---+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "[('raceId', 'int'), ('driverId', 'int'), ('pit_stop', 'int'), ('lap', 'int'), ('stop_duration', 'double')]\n"
     ]
    }
   ],
   "source": [
    "pit_stops_df = spark.read.csv(pit_stops, header=True, inferSchema=True)\n",
    "pit_stops_df = pit_stops_df.drop('milliseconds', 'time')\n",
    "pit_stops_df = pit_stops_df.withColumnRenamed('duration', 'stop_duration')\n",
    "pit_stops_df = pit_stops_df.withColumnRenamed('stop', 'pit_stop')\n",
    "pit_stops_df = pit_stops_df.withColumn('stop_duration', col('stop_duration').cast(DoubleType()))\n",
    "pit_stops_df.show()\n",
    "print(pit_stops_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+--------+-------------+\n",
      "|driverStandingsId|raceId|driverId|driver_points|\n",
      "+-----------------+------+--------+-------------+\n",
      "|                1|    18|       1|         10.0|\n",
      "|                2|    18|       2|          8.0|\n",
      "|                3|    18|       3|          6.0|\n",
      "|                4|    18|       4|          5.0|\n",
      "|                5|    18|       5|          4.0|\n",
      "|                6|    18|       6|          3.0|\n",
      "|                7|    18|       7|          2.0|\n",
      "|                8|    18|       8|          1.0|\n",
      "|                9|    19|       1|         14.0|\n",
      "|               10|    19|       2|         11.0|\n",
      "|               11|    19|       3|          6.0|\n",
      "|               12|    19|       4|          6.0|\n",
      "|               13|    19|       5|         10.0|\n",
      "|               14|    19|       6|          3.0|\n",
      "|               15|    19|       7|          2.0|\n",
      "|               16|    19|       8|         11.0|\n",
      "|               17|    19|       9|          8.0|\n",
      "|               18|    19|      15|          5.0|\n",
      "|               19|    19|      17|          2.0|\n",
      "|               20|    19|      14|          0.0|\n",
      "+-----------------+------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "[('driverStandingsId', 'int'), ('raceId', 'int'), ('driverId', 'int'), ('driver_points', 'double')]\n"
     ]
    }
   ],
   "source": [
    "# driver position is driver's championship standing\n",
    "driver_standings_df = spark.read.csv(driver_standings, header=True, inferSchema=True)\n",
    "driver_standings_df = driver_standings_df.drop('position','positionText', 'wins')\n",
    "driver_standings_df = driver_standings_df.withColumnRenamed('points', 'driver_points')\n",
    "driver_standings_df.show()\n",
    "print(driver_standings_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+\n",
      "|raceId|circuitId|        circuit_name|\n",
      "+------+---------+--------------------+\n",
      "|     1|        1|Australian Grand ...|\n",
      "|     2|        2|Malaysian Grand Prix|\n",
      "|     3|       17|  Chinese Grand Prix|\n",
      "|     4|        3|  Bahrain Grand Prix|\n",
      "|     5|        4|  Spanish Grand Prix|\n",
      "|     6|        6|   Monaco Grand Prix|\n",
      "|     7|        5|  Turkish Grand Prix|\n",
      "|     8|        9|  British Grand Prix|\n",
      "|     9|       20|   German Grand Prix|\n",
      "|    10|       11|Hungarian Grand Prix|\n",
      "|    11|       12| European Grand Prix|\n",
      "|    12|       13|  Belgian Grand Prix|\n",
      "|    13|       14|  Italian Grand Prix|\n",
      "|    14|       15|Singapore Grand Prix|\n",
      "|    15|       22| Japanese Grand Prix|\n",
      "|    16|       18|Brazilian Grand Prix|\n",
      "|    17|       24|Abu Dhabi Grand Prix|\n",
      "|    18|        1|Australian Grand ...|\n",
      "|    19|        2|Malaysian Grand Prix|\n",
      "|    20|        3|  Bahrain Grand Prix|\n",
      "+------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "races_df = spark.read.csv(races, header=True, inferSchema=True)\n",
    "races_df = races_df.drop('year','date','time', 'round','url', 'fp1_date', 'fp1_time', 'fp2_date', 'fp2_time', 'fp3_date', 'fp3_time', 'quali_date', 'quali_time', 'sprint_date', 'sprint_time')\n",
    "races_df = races_df.withColumnRenamed('name', 'circuit_name')\n",
    "races_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+\n",
      "|constructorId|constructor_name|\n",
      "+-------------+----------------+\n",
      "|            1|         McLaren|\n",
      "|            2|      BMW Sauber|\n",
      "|            3|        Williams|\n",
      "|            4|         Renault|\n",
      "|            5|      Toro Rosso|\n",
      "|            6|         Ferrari|\n",
      "|            7|          Toyota|\n",
      "|            8|     Super Aguri|\n",
      "|            9|        Red Bull|\n",
      "|           10|     Force India|\n",
      "|           11|           Honda|\n",
      "|           12|          Spyker|\n",
      "|           13|             MF1|\n",
      "|           14|      Spyker MF1|\n",
      "|           15|          Sauber|\n",
      "|           16|             BAR|\n",
      "|           17|          Jordan|\n",
      "|           18|         Minardi|\n",
      "|           19|          Jaguar|\n",
      "|           20|           Prost|\n",
      "+-------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "constructors_df = spark.read.csv(constructors, header=True, inferSchema=True)\n",
    "constructors_df = constructors_df.drop('url', 'nationality', 'constructorRef')\n",
    "constructors_df = constructors_df.withColumnRenamed('name', 'constructor_name')\n",
    "constructors_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|statusId|      status|\n",
      "+--------+------------+\n",
      "|       1|    Finished|\n",
      "|       2|Disqualified|\n",
      "|       3|    Accident|\n",
      "|       4|   Collision|\n",
      "|       5|      Engine|\n",
      "|       6|     Gearbox|\n",
      "|       7|Transmission|\n",
      "|       8|      Clutch|\n",
      "|       9|  Hydraulics|\n",
      "|      10|  Electrical|\n",
      "|      11|      +1 Lap|\n",
      "|      12|     +2 Laps|\n",
      "|      13|     +3 Laps|\n",
      "|      14|     +4 Laps|\n",
      "|      15|     +5 Laps|\n",
      "|      16|     +6 Laps|\n",
      "|      17|     +7 Laps|\n",
      "|      18|     +8 Laps|\n",
      "|      19|     +9 Laps|\n",
      "|      20|    Spun off|\n",
      "+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we should filter data based on every driver that has finished\n",
    "status_df = spark.read.csv(status, header=True, inferSchema=True)\n",
    "status_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+---+\n",
      "|circuitId|     lat|      lng|alt|\n",
      "+---------+--------+---------+---+\n",
      "|        1|-37.8497|  144.968| 10|\n",
      "|        2| 2.76083|  101.738| 18|\n",
      "|        3| 26.0325|  50.5106|  7|\n",
      "|        4|   41.57|  2.26111|109|\n",
      "|        5| 40.9517|   29.405|130|\n",
      "|        6| 43.7347|  7.42056|  7|\n",
      "|        7|    45.5| -73.5228| 13|\n",
      "|        8| 46.8642|  3.16361|228|\n",
      "|        9| 52.0786| -1.01694|153|\n",
      "|       10| 49.3278|  8.56583|103|\n",
      "|       11| 47.5789|  19.2486|264|\n",
      "|       12| 39.4589|-0.331667|  4|\n",
      "|       13| 50.4372|  5.97139|401|\n",
      "|       14| 45.6156|  9.28111|162|\n",
      "|       15|  1.2914|  103.864| 18|\n",
      "|       16| 35.3717|  138.927|583|\n",
      "|       17| 31.3389|   121.22|  5|\n",
      "|       18|-23.7036| -46.6997|785|\n",
      "|       19|  39.795| -86.2347|223|\n",
      "|       20| 50.3356|   6.9475|578|\n",
      "+---------+--------+---------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I don't think we need this dataset unless we want to include the lat, lng and alt \n",
    "# of each circuit when it comes to performance but that might be difficult\n",
    "# plus we already have circuit_id in race\n",
    "circuits_df = spark.read.csv(circuits, header=True, inferSchema=True)\n",
    "circuits_df = circuits_df.drop('circuitRef', 'name', 'location', 'url', 'country')\n",
    "circuits_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as db\n",
    "import pandas as pd\n",
    "\n",
    "conn = db.connect('database.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
